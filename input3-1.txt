Introduction
Web Search Engine using PageRank: Google
The two papers propose the PageRank algorithm and how it is used to build an efficient search engine that we extensively use today – Google. The authors start by explaining the disadvantages of the search engines that were available before Google and continue to address these issues in the implementation of the Google Search Engine. The existing search engines at the time were built on the fact that a complete search index would make it possible to find anything easily. But the truth is that, a larger search index just returns a large number of documents in no particular order of relevance. When searching for something, people only go through the first ten documents or so. So, it has to be made sure that the top ten documents returned have high precision and relevance to the query. Google achieved this by using the hypertextual information, such as link structure and anchor text to produce a global importance ranking of every page.
Problems addressed
The main goals of the Google Web Search Engine can be summarized as follows: Google was designed to scale for large magnitude of data that was available in the web. It aimed to build a practical large-scale system which can exploit additional information present in hypertext to produce better search results. In order to accomplish the above goals, Google made use of a fast crawling technology to gather the web documents and kept them up-to-date. Efficient storage techniques and indexing systems were used to process large amount of data efficiently. Query processing was implemented to process hundreds to thousands of queries per second.
Proposed solution
The following constructs were considered in the design of the PageRank algorithm: Every page has some forward links which can be determined by downloading the page and observing its contents, and backlinks which are not so easy to determine and found iteratively. Highly linked pages are more important than pages with fewer links. But, this design consideration does not suffice. We need to take into account the fact that some pages may be more important than

others. A simple solution proposed by Google is that, a page has high rank if the sum of the ranks of its backlinks is high.
 presents the following formula to find the PageRank of a web page: Consider that Page A has pages Page A has pages T1...Tn which point to it. The parameter d is a damping factor which can be set between 0 and 1 (usually set to 0.85). C(A) is defined as the number of links going out of page A. The PageRank of a page A is given as follows:
presents a similar approach for PageRank calculation: Let u be the web page. Fu be the set of pages u points to and Bu be the set of pages that point to u. Let Nu = |Fu| be the number of links from u and let c be a factor used for normalization. A simplified version of PageRank R is defined as follows:
shows the PageRank of the three pages after convergence) PageRank calculation: Convert each URL into a unique integer and store each hyperlink in a database using the integer IDs to identify pages. Then sort the link structure by Parent ID. The
  
dangling pointers (pages with no forward links) are then eliminated. Perform initial assignment of ranks/weights. This has to be done carefully in order for the weights to converge sooner. Continue this till the weights for every page converge. Add the dangling links back and recompute the PageRank. The working of the PageRank algorithm can be explained using the “random surfer” model described. This model is based on the behavior of a random surfer who clicks on a web page at random and keeps following the links on the page until he gets bored, at which point he skips to another random page. The probability that the random surfer visits a page is its PageRank. And, the d damping factor is the probability at which he will get bored and click on another random page. A page can have a high PageRank if there are many pages that point to it, or if there are some pages that point to it and have a high PageRank. PageRank handles both these cases and everything in between by recursively propagating weights through the link structure of the web.
Page Rank Architecture:

Strengths and weaknesses
Google uses several distributed crawlers to download the huge number of pages available on the web. Since web crawling is the most time-consuming step in computing PageRank, this distributed architecture makes it faster. Each crawler maintains its own DNS cache, so it does not need to do a DNS lookup before crawling each document. The Store Server then compresses and stores the web pages into a repository, to make it space efficient. Google's data structures are optimized so that a large document collection can be crawled, indexed, and searched with lesser cost. Google’s data structures are designed to avoid disk seeks whenever possible. Virtual files spanning multiple file systems called BigFiles used in Google provide support for basic compression options. Google’s repository contains the full HTML of every web page. Each page is compressed using zlib, which provides a good tradeoff between speed and compression ratio. In the repository, the documents are stored one after the other and are prefixed by docID, length, and URL which helps with data consistency and makes development and querying much easier. Hit lists which record a list of occurrences of a word in a particular document, account for most of the space used in both the forward and the inverted indices, and hence need to be stored efficiently. Google used a hand optimized compact encoding method since it requires far less space than the simple encoding and far less bit manipulation than Huffman coding. Using flex, they wrote a lexical analyzer which could handle a huge amount of errors, which could not be done by existing systems. Indexing documents into barrels and sorting of documents was done parallelly.
Despite of all this, Google faced a number of issues with their implementation. People were unaware of what a crawler is and started sending emails to check if Google liked their content since they had viewed it. People included statements like "This page is copyrighted and should not be indexed", to protect their pages from being crawled, which needless to say is difficult for web crawlers to understand. When crawling an online application, the PageRank algorithm generated garbage data and disturbed the normal execution of the application.
Some of the applications of this PageRank algorithm are described in. Estimating web traffic – It is observed that PageRank doesn’t behave well. This is explained by people’s behavior to not

cite things they want to hide. Also, some backlinks are simply omitted from the database, because they started with an initial structure (75 million URL database), which is only a partial link structure of the web. One other application is using PageRank as a Backlink predictor. Experiments show that PageRank provides better citation count approximation than citation counts itself, because of the Random Surfer model of PageRank. Citation counting get stuck in local collections, taking a long time to branch out and find highly cited pages in other areas. PageRank can be used for providing User Navigation using the PageRank proxy, which is an application that annotates each page that a user sees with its PageRank. This way the user knows the importance of the pages he is going to visit beforehand.
Conclusion
Both these papers provide detailed information about how Google’s PageRank Algorithm came into existence. The initial design described in these papers, has its own pros and cons. Heavy importance was given to the the proximity of word occurrences, unlike the other search engines in existence at that time. It was designed to returns non-crawlable data such as images, email addresses because of use of anchor texts. It used compression to store the data and hence requires lesser storage. While the goal of Google was to provide quality search results efficiently, the initial implementation of Google concentrated more on quality. It did not pay a lot of attention to the query processing itself. Answering queries took between 1 to 10 seconds. Google did not have any optimizations such as query caching, sub-indices on common terms, and other common optimizations. They also faced bottlenecks in CPU, memory access, memory capacity, disk seeks, disk throughput, disk capacity, and network IO. This being the situation in the 90’s, Google has evolved over the years to be the most used search engine in the world. It has 8 datacenters worldwide and can return queries in a fraction of a second!
